# MSM Project — Monte Carlo Metropolis Algorithm

This project explores how efficiently a Monte Carlo Metropolis algorithm can approximate the Markov State Matrix (MSM) by comparing a Monte Carlo estimation against an exact analytical calculation.

## Overview

The algorithm constructs a Markov transition matrix using the Metropolis-Hastings algorithm on a weight distribution. It compares two methods:
- **Explicit calculation**: Analytically computes the full transition matrix
- **Monte Carlo estimation**: Estimates the transition matrix through random sampling

The convergence quality is measured using χ² (chi-squared) distance between the estimated and explicit matrices.

## How to Run

Simply run the main script in MATLAB:

```matlab
main
```

The script will prompt you for various configuration options (described below), then execute the algorithm and display results.

## Configuration Options

### 1. Weight Distribution

**Prompt:** `Do you want to use the teacher's weights? (Y/N):`

- **Y (Yes)**: Uses a 2D Gaussian weight distribution generated by `MetroW.m`
  - Follow-up prompt: `What edge size do you want to use? (positive integer):`
  - Defines the size of the square grid (e.g., 20 creates a 20×20 grid)
  
- **N (No)**: Uses pre-generated Perlin noise distributions
  - Follow-up prompt: `What dimension dataset do you want to use? (1, 2, or 3):`
  - **1**: 1D Perlin noise (`perlin_noise1d.mat`) - animation/visualization not supported
  - **2**: 2D Perlin noise (`perlin_noise2d.mat`) - displays a visualization
  - **3**: 3D Perlin noise (`perlin_noise3d.mat`) - animation/visualization not supported

### 2. Proposal Method

**Prompt:** `Do you want nearest neighbour (Y) or any neighbour (N):`

- **Y (Nearest Neighbour)**: 
  - Proposals are only made to adjacent states (up/down/left/right in grid)
  - Uses `build_markov_explicit_nn.m` and `build_monte_carlo_nearest.m`
  
- **N (Any Neighbour)**: 
  - Proposals can be made to any state in the system
  - Uses `build_markov_explicit.m` and `build_monte_carlo.m`

### 3. Visualization

**Prompt:** `Do you want animations at the cost of performance (Y/N)?`

- **Y (Yes)**: 
  - Displays real-time animation during Monte Carlo simulation
  - Shows state visit counts and χ² convergence
  - At completion, shows side-by-side comparison of explicit vs. Monte Carlo matrices
  
- **N (No)**: 
  - Shows only a progress bar during computation
  - Faster execution
  - Still displays final χ² convergence plot

### 4. Number of Iterations

**Prompt:** `At what iteration do you want to stop at (int)?`

- Enter the number of Monte Carlo steps to run
- More iterations = better convergence but longer runtime
- Typical values: 10,000 to 1,000,000 depending on system size

### 5. Save Results

**Prompt:** `Enter N to skip saving, or type a folder name to save data:`

- **N**: Does not save any results (useful for quick tests)
- **[folder_name]**: Saves all results to `output/[folder_name]/`
  - If folder exists, automatically appends `_1`, `_2`, etc.
  
**Saved files:**
- `M_explicit.mat`: Analytically calculated transition matrix
- `M.mat`: Monte Carlo estimated transition matrix
- `W.mat`: Weight distribution used
- `parameters.mat`: Algorithm parameters (nearest_neighbour flag)
- `convergence_params.mat`: Number of iterations
- `chi_history.mat`: χ² values at each step

## Output

The algorithm produces the following visualizations:

1. **χ² Convergence Plot** (log-log scale): Shows how the estimation error decreases with iterations
2. **2D Perlin Noise Distribution** (if applicable): Visualization of the weight distribution
3. **State Visit Counts** (if animations enabled): Heat map showing which states were visited during sampling
4. **Matrix Comparison** (if animations enabled): Side-by-side comparison of log₁₀(M_explicit) vs log₁₀(M)

## Algorithm Details

### Metropolis-Hastings Algorithm

For each step:
1. Start at current state `i`
2. Propose a new state `j` according to proposal distribution `q(i→j)`:
   - **Any neighbour**: Uniform over all N-1 other states, `q(i→j) = 1/(N-1)`
   - **Nearest neighbour**: Uniform over k adjacent states, `q(i→j) = 1/k` where k ≤ 4
3. Accept the proposal with probability: `α = min(1, w(j)/w(i))` 
   - Note: The q terms cancel since the proposal is symmetric: `q(i→j) = q(j→i)`
4. If accepted, move to state `j`; otherwise stay at state `i`
5. Update transition count matrix

### Convergence Metric

The χ² distance is calculated as:

$$\chi^2 = \sum_{i,j: M_{explicit}(i,j) > 0} \frac{(M_{MC}(i,j) - M_{explicit}(i,j))^2}{M_{explicit}(i,j)}$$

This measures the quality of the Monte Carlo approximation relative to the exact solution.

## Example Usage

```matlab
>> main
Do you want to use the teacher's weights? (Y/N): Y
What edge size do you want to use? (positive integer): 50
Do you want nearest neighbour (Y) or any neighbour (N): Y
Do you want animations at the cost of performance (Y/N)? N
At what iteration do you want to stop at (int)? 100000
Enter N to skip saving, or type a folder name to save data: test_run
```

This runs a 50×50 grid with nearest neighbour proposals, 100,000 iterations, and saves results to `output/test_run/`.